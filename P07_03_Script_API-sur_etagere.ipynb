{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acting-purse",
   "metadata": {},
   "source": [
    "# Utilisation de l'API Azure Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-authorization",
   "metadata": {},
   "source": [
    "#### 1-Lecture de la clé d'API et instantiation du client virtuel : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "classified-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=open(\"macle.txt\",\"r\")\n",
    "key = k.readline()\n",
    "endpoint = \"https://apita.cognitiveservices.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "careful-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prerequisite-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\\\Users\\\\Wince\\\\Downloads\\\\OC\\\\Projet_7\\\\data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-interface",
   "metadata": {},
   "source": [
    "#### 2-Importation du fichier contenant les twits à tester : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-monkey",
   "metadata": {},
   "source": [
    "Ouverture du fichier échantillon de test et preprocessing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apart-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lewin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports_done\n",
      "Text preprocessing initiated\n",
      "string cleanup in progress...\n",
      "string cleanup OK, now tokenizing\n",
      "Tokenizing OK - now lemmatizing...\n",
      "Lemmatizing OK\n",
      "Text preprocessing finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"imports_done\")\n",
    "\n",
    "def string_cleanup(text):\n",
    "    output = contractions.fix(text)\n",
    "    output = re.sub(r'http?://\\S+', '', output, flags=re.MULTILINE)\n",
    "    output = re.sub(r'@\\w+', '', output, flags=re.MULTILINE)\n",
    "    output = [char.lower() if char not in string.punctuation else ' ' for char in output ]\n",
    "    output = ''.join(output)\n",
    "    return output\n",
    "\n",
    "def lemmatize(tokenlist): \n",
    "    wnl = WordNetLemmatizer()\n",
    "    out = [wnl.lemmatize(word) for word in tokenlist]\n",
    "    return out    \n",
    "\n",
    "def text_preprocessing(df):\n",
    "    print('Text preprocessing initiated')\n",
    "    print('string cleanup in progress...')\n",
    "    df.loc[:,'cleaned_text'] = df.text.apply(string_cleanup)\n",
    "    print('string cleanup OK, now tokenizing')\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    df.loc[:, 'tokens'] = df.loc[\n",
    "        :, 'cleaned_text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    print('Tokenizing OK - now lemmatizing...')\n",
    "    df.loc[:, 'lems'] = df.loc[:, 'tokens'].apply(lemmatize)\n",
    "    df.loc[:, 'lems'] = df.lems.apply(' '.join)\n",
    "    print('Lemmatizing OK')\n",
    "    print('Text preprocessing finished')\n",
    "    return df\n",
    "\n",
    "\n",
    "test_sample = pd.read_csv(r'C:\\\\Users\\\\Lewin\\\\Downloads\\\\OC\\\\Projet_7\\\\upload\\\\test_file.csv')\n",
    "test_sample = text_preprocessing(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suspended-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194030</td>\n",
       "      <td>In bed. Not wanting to do a 10 hour shift toda...</td>\n",
       "      <td>0</td>\n",
       "      <td>in bed  not wanting to do a 10 hour shift toda...</td>\n",
       "      <td>[in, bed, not, wanting, to, do, a, 10, hour, s...</td>\n",
       "      <td>in bed not wanting to do a 10 hour shift today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427946</td>\n",
       "      <td>@Shontelle_Layne Wish i could go!</td>\n",
       "      <td>0</td>\n",
       "      <td>wish i could go</td>\n",
       "      <td>[wish, i, could, go]</td>\n",
       "      <td>wish i could go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>761589</td>\n",
       "      <td>Really needs to start tanning the RIGHT arm ON...</td>\n",
       "      <td>0</td>\n",
       "      <td>really needs to start tanning the right arm on...</td>\n",
       "      <td>[really, needs, to, start, tanning, the, right...</td>\n",
       "      <td>really need to start tanning the right arm onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701510</td>\n",
       "      <td>Hey @officialTila we lost you on trending topics</td>\n",
       "      <td>0</td>\n",
       "      <td>hey  we lost you on trending topics</td>\n",
       "      <td>[hey, we, lost, you, on, trending, topics]</td>\n",
       "      <td>hey we lost you on trending topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166150</td>\n",
       "      <td>Venus Williams is having a horrible day at the...</td>\n",
       "      <td>0</td>\n",
       "      <td>venus williams is having a horrible day at the...</td>\n",
       "      <td>[venus, williams, is, having, a, horrible, day...</td>\n",
       "      <td>venus williams is having a horrible day at the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  sentiment  \\\n",
       "0      194030  In bed. Not wanting to do a 10 hour shift toda...          0   \n",
       "1      427946                 @Shontelle_Layne Wish i could go!           0   \n",
       "2      761589  Really needs to start tanning the RIGHT arm ON...          0   \n",
       "3      701510  Hey @officialTila we lost you on trending topics           0   \n",
       "4      166150  Venus Williams is having a horrible day at the...          0   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  in bed  not wanting to do a 10 hour shift toda...   \n",
       "1                                  wish i could go     \n",
       "2  really needs to start tanning the right arm on...   \n",
       "3               hey  we lost you on trending topics    \n",
       "4  venus williams is having a horrible day at the...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [in, bed, not, wanting, to, do, a, 10, hour, s...   \n",
       "1                               [wish, i, could, go]   \n",
       "2  [really, needs, to, start, tanning, the, right...   \n",
       "3         [hey, we, lost, you, on, trending, topics]   \n",
       "4  [venus, williams, is, having, a, horrible, day...   \n",
       "\n",
       "                                                lems  \n",
       "0  in bed not wanting to do a 10 hour shift today...  \n",
       "1                                    wish i could go  \n",
       "2  really need to start tanning the right arm onl...  \n",
       "3                  hey we lost you on trending topic  \n",
       "4  venus williams is having a horrible day at the...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample.head()#[1290:1300].cleaned_text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "overall-justice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-large",
   "metadata": {},
   "source": [
    "#### 3-Envoi de requêtes au service : \n",
    "On fait d'abord un essai sur la ou les premières lignes du jeu de données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vocational-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AnalyzeSentimentResult(id=0, sentiment=positive, warnings=[], statistics=None, confidence_scores=SentimentConfidenceScores(positive=0.87, neutral=0.02, negative=0.11), sentences=[SentenceSentiment(text=in bed  not wanting to do a 10 hour shift today  well needs must x, sentiment=positive, confidence_scores=SentimentConfidenceScores(positive=0.87, neutral=0.02, negative=0.11))], is_error=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "document = []\n",
    "for n in range(1):\n",
    "    document.append({\"id\": str(n), \"language\": \"en\", \"text\":test_sample.loc[n,'cleaned_text']})\n",
    "response = client.analyze_sentiment(document)\n",
    "successful_responses = [doc for doc in response if not doc.is_error ]\n",
    "successful_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-preview",
   "metadata": {},
   "source": [
    "##### Envoi de la requête à l'API, récupération des résultats puis mise en forme pour l'évaluation de la performance prédictive du modèle sur étagère : <br>\n",
    "\n",
    "Suite à des plantages répétés, un code de gestion des erreurs a été mis en place afin d'en identifier les causes ( caractères exotique et texte absent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controlled-socket",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1595 in progress\n",
      "retrying indices : [272, 1534]\n",
      "Index 272 has no content and can not be processed\n",
      "Index 1534 has no content and can not be processed\n",
      "following indices could not be analyzed : [272, 1534]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "successful_responses = []\n",
    "for n in range(0,len(test_sample),5):   \n",
    "    clear_output(wait=True)\n",
    "    print(f'run {n} in progress')\n",
    "    document = []\n",
    "    for i in range(n,n+5):\n",
    "        if len(test_sample.cleaned_text[test_sample.index[i]]) > 1:\n",
    "            document.append({\"id\": str(i), \"language\": \"en\", \"text\":test_sample.cleaned_text[test_sample.index[i]]})\n",
    "        else: \n",
    "            print(f'Index {i} has no content and can not be processed') \n",
    "    response = client.analyze_sentiment(document)\n",
    "    successful_responses = successful_responses + [doc for doc in response if not doc.is_error ]\n",
    "    time.sleep(0.2)\n",
    "successful_indices = np.unique([int(doc.id) for doc in successful_responses])\n",
    "if len(successful_indices) < test_sample.shape[0]: \n",
    "    full = [i for i in range(1600)]\n",
    "    list_difference = []\n",
    "    for item in full:\n",
    "        if item not in successful_indices:\n",
    "            list_difference.append(item)\n",
    "    print(f'retrying indices : {list_difference}')\n",
    "    for i in list_difference:\n",
    "        if len(test_sample.cleaned_text[test_sample.index[i]]) > 1:\n",
    "            document.append({\"id\": str(i), \"language\": \"en\", \"text\":test_sample.cleaned_text[test_sample.index[i]]})\n",
    "            successful_responses = successful_responses + [doc for doc in response if not doc.is_error ]\n",
    "        else: \n",
    "            print(f'Index {i} has no content and can not be processed') \n",
    "    successful_indices = np.unique([int(doc.id) for doc in successful_responses])\n",
    "if len(successful_indices) < test_sample.shape[0]: \n",
    "    full = [i for i in range(1600)]\n",
    "    list_difference = []\n",
    "    for item in full:\n",
    "        if item not in successful_indices:\n",
    "            list_difference.append(item)\n",
    "    print(f'following indices could not be analyzed : {list_difference}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-interference",
   "metadata": {},
   "source": [
    "#### 4- Evaluation des réponses : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-crack",
   "metadata": {},
   "source": [
    "Il nous manque certaines lignes dont le nettoyage a supprimé l'intégralité du texte : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innovative-bidder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(successful_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-morrison",
   "metadata": {},
   "source": [
    "On va donc construire notre vecteur de référence en éliminant les lignes non-analysées : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "processed-opportunity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_sample.loc[[int(doc.id) for doc in successful_responses],'sentiment'].to_numpy()\n",
    "len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-sarah",
   "metadata": {},
   "source": [
    "Puis le vecteur de prédictions, binarisé en considérant uniquement le score de confiance sur la classe négative : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "infectious-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[]\n",
    "y_proba = []\n",
    "y_pred = []\n",
    "for doc in successful_responses:\n",
    "    y_proba.append(doc.confidence_scores.negative)\n",
    "    indices.append(doc.id)\n",
    "    y_pred.append(0 if doc.confidence_scores.negative >= 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mobile-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extraordinary-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred=np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-embassy",
   "metadata": {},
   "source": [
    "Comparaison des prédictions aux étiquettes du dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "close-tribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[449, 369],\n",
       "       [ 95, 685]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "appreciated-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>449</td>\n",
       "      <td>369</td>\n",
       "      <td>95</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tn   fp  fn   tp\n",
       "0  449  369  95  685"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_true, y_pred).ravel().reshape(1,-1), columns=[\"tn\", \"fp\", \"fn\", \"tp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "inside-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC du modèle API sur étagère : 0.7135524418531753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(f'ROC_AUC du modèle API sur étagère : {roc_auc_score(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "charged-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC du modèle API sur étagère : 0.7096370463078848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Exactitude du modèle API sur étagère : {accuracy_score(y_true, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
