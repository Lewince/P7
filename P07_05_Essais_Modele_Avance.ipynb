{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "featured-damages",
   "metadata": {},
   "source": [
    "# Essais d'architectures de réseaux profonds pour analyse de sentiment sur tweets\n",
    "\n",
    "#### Import et préparation de données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "narrow-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wince\\Downloads\\OC\\Projet_7\\data\n"
     ]
    }
   ],
   "source": [
    "cd C:\\\\Users\\\\Wince\\\\Downloads\\\\OC\\\\Projet_7\\\\data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fuzzy-henry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>twit_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment     twit_id                      datetime     query  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user_id                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", header=None,\n",
    "                      names=[\"sentiment\", \"twit_id\", \"datetime\", \"query\", \"user_id\",\"text\"],\n",
    "                      encoding = \"ISO-8859-1\")\n",
    "twit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "crude-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df.loc[:,'sentiment'] = twit_df.sentiment.apply(lambda x : 1 if x > 2 else 0)\n",
    "twit_df = twit_df.loc[:,['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "trained-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def string_cleanup(text):\n",
    "    output = contractions.fix(text)\n",
    "    output = re.sub(r'http?://\\S+', '', output, flags=re.MULTILINE)\n",
    "    output = re.sub(r'@\\w+', '', output, flags=re.MULTILINE)\n",
    "    output = [char for char in output if char not in string.punctuation]\n",
    "    output = ''.join(output)\n",
    "    return output\n",
    "\n",
    "twit_df.loc[:,'cleaned_text'] = twit_df.text.apply(string_cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rental-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "twit_df.loc[:, 'tokens'] = twit_df.loc[\n",
    "    :, 'cleaned_text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proper-casino",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Wince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize(tokenlist): \n",
    "    out = [wnl.lemmatize(word) for word in tokenlist]\n",
    "    return out\n",
    "        \n",
    "twit_df.loc[:, 'lems'] = twit_df.loc[\n",
    "    :, 'tokens'].apply(lemmatize)\n",
    "twit_df.loc[:, 'lems_sequence'] = twit_df.lems.apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-supervisor",
   "metadata": {},
   "source": [
    "Les essais se conduiront sur la variable sélectionnée ci-dessous : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comfortable-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column = 'cleaned_text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-advice",
   "metadata": {},
   "source": [
    "Nettoyage argot et jargon avant vectorisation : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-nepal",
   "metadata": {},
   "source": [
    "On va regarder s'il existe des termes à nettoyer parmi les plus fréquents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "western-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for corpus in twit_df.loc[:, 'tokens']:\n",
    "    for token in corpus:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nutritional-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>985578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>615938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>520361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>391503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>377702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>deal</td>\n",
       "      <td>1773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>l</td>\n",
       "      <td>1772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ohh</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>eh</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>wife</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   count\n",
       "0       i  985578\n",
       "1      to  615938\n",
       "2     the  520361\n",
       "3      is  391503\n",
       "4       a  377702\n",
       "..    ...     ...\n",
       "995  deal    1773\n",
       "996     l    1772\n",
       "997   ohh    1768\n",
       "998    eh    1768\n",
       "999  wife    1768\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 1000\n",
    "most_frequent = sorted(wordfreq, key=wordfreq.get, reverse=True)[:n_features]\n",
    "word_counts = [wordfreq[i] for i in sorted(\n",
    "    wordfreq, key=wordfreq.get, reverse=True)][:n_features]\n",
    "word_counts = pd.concat([pd.Series(most_frequent),\n",
    "                        pd.Series(word_counts)], axis=1)\n",
    "word_counts = word_counts.rename(columns={0: 'word', 1: 'count'})\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sixth-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bibtexparser\n",
    "# from bibtexparser.bparser import BibTexParser\n",
    "# btp = BibTexParser()\n",
    "# with open('C:/Users/Wince/Downloads/OC/Projet_7/Modele_avance/slang_dict_en.bib') as bibtex_file:\n",
    "#     bib_database = bibtexparser.load(bibtex_file, parser = btp)\n",
    "\n",
    "# print(bib_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-volunteer",
   "metadata": {},
   "source": [
    "Train/test Split façon Keras (on mélange et on splitte sur les indexs) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "breathing-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    " twit_df = twit_df.sample(frac=1)\n",
    "\n",
    "validation_split = 0.2\n",
    "\n",
    "num_validation_samples = int(validation_split * len(twit_df))\n",
    "train_samples = twit_df[:-num_validation_samples][\n",
    "    twit_df.columns[~twit_df.columns.str.contains('sentiment')]]\n",
    "val_samples = twit_df[-num_validation_samples:][\n",
    "    twit_df.columns[~twit_df.columns.str.contains('sentiment')]]\n",
    "train_labels = twit_df[:-num_validation_samples].sentiment\n",
    "val_labels = twit_df[-num_validation_samples:].sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-garden",
   "metadata": {},
   "source": [
    "#### Vectorisation avec l'outil Keras et création de la couche d'embedding GloVe : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-welding",
   "metadata": {},
   "source": [
    "Voyons la longueur du commentaire le plus long : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "welcome-suicide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(twit_df.tokens, key=len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-device",
   "metadata": {},
   "source": [
    "On va donc construire nos tenseurs en paddant notre séquence à 120 mots (ça laisse une petite marge pour l'inférence :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medium-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done in 8.947515726089478 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "start_time = time.time()\n",
    "vectorizer = TextVectorization(max_tokens=50000, output_sequence_length=120)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples.loc[:,selected_column]).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "print(\"--- Done in %s seconds ---\" % (time.time() - start_time))\n",
    "def _get_vocabulary():\n",
    "    keyz, valuez = vectorizer._index_lookup_layer._table_handler.data()\n",
    "    return [x.decode('latin-1') for _,x in sorted(zip(valuez, keyz))]\n",
    "vocab = _get_vocabulary()\n",
    "my_word_index = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-blackjack",
   "metadata": {},
   "source": [
    "On charge notre dictionnaire GloVe puis on crée le noyau de notre embedding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "filled-hebrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wince\\Downloads\\OC\\Projet_7\\Modele_avance\n"
     ]
    }
   ],
   "source": [
    "cd C:/Users/Wince/Downloads/OC/Projet_7/Modele_avance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "renewable-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-5cf75fee856f>:6: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  vector = np.fromstring(values, \"f\", sep=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Glove vectors loaded in 81.9329743385315 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "glove_dict = {}\n",
    "with open(\"glove.twitter.27B.200d.txt\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    for line in f:\n",
    "        word, values = line.split(maxsplit=1)\n",
    "        vector = np.fromstring(values, \"f\", sep=\" \")\n",
    "        glove_dict[word] = vector\n",
    "print(\"---Glove vectors loaded in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moving-profession",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118763"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "facial-permit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 40883 words (9116 misses)\n",
      "--- Done in 0.08707880973815918 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_embedding_matrix(num_tokens = len(vocab)+2,\n",
    "                            embedding_dim = 200,\n",
    "                            my_word_index=my_word_index,\n",
    "                            dictionary=glove_dict):\n",
    "    start_time = time.time()\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in my_word_index.items():\n",
    "        embedding_vector = dictionary.get(word)\n",
    "        if embedding_vector is not None and len(embedding_vector)!=0:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    print(\"--- Done in %s seconds ---\" % (time.time() - start_time))\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unlimited-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(vocab)+2, 200,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-newark",
   "metadata": {},
   "source": [
    "Formatage des données d'entraînement/validation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "early-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples.loc[:,selected_column]])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples.loc[:,selected_column]])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "manufactured-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "developed-chemical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280000, 120)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "nonprofit-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 120)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[:1600].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-current",
   "metadata": {},
   "source": [
    "#### Création et évaluation d'un modèle Baseline : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-african",
   "metadata": {},
   "source": [
    "Vu la taille des données, on utilisera un classifieur SGD en perte log plutôt qu'une régression logistique (le problème résolu est le même mais la méthode est plus adaptée). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "worst-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cv = CountVectorizer(max_features=50000)\n",
    "X_train = cv.fit_transform(train_samples.loc[:,selected_column])\n",
    "X_test = cv.transform(val_samples.loc[:,selected_column])\n",
    "sgd = SGDClassifier(loss='log')\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test[:1600])\n",
    "confusion = confusion_matrix(y_val[:1600], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "italian-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>617</td>\n",
       "      <td>175</td>\n",
       "      <td>152</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tn   fp   fn   tp\n",
       "0  617  175  152  656"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion.ravel().reshape(1,-1), columns=[\"tn\", \"fp\", \"fn\", \"tp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "appointed-cross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7954607960796078"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(y_val[:1600], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "related-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_val[:1600], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-popularity",
   "metadata": {},
   "source": [
    "On entraîne aussi un classifieur naif Bayésien : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "coated-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test[:1600])\n",
    "confusion = confusion_matrix(y_val[:1600], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "pacific-harvey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>636</td>\n",
       "      <td>156</td>\n",
       "      <td>183</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tn   fp   fn   tp\n",
       "0  636  156  183  625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion.ravel().reshape(1,-1), columns=[\"tn\", \"fp\", \"fn\", \"tp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abstract-christianity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7882725772577258"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(y_val[:1600], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "handy-twins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.788125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_val[:1600], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-humanitarian",
   "metadata": {},
   "source": [
    "#### Modèle basique (1 couche lstm) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "golden-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         10000200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         168448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 10,168,777\n",
      "Trainable params: 168,577\n",
      "Non-trainable params: 10,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "proprietary-lecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 1366s 136ms/step - loss: 0.5695 - acc: 0.7013 - val_loss: 0.4587 - val_acc: 0.7782\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 1358s 136ms/step - loss: 0.4462 - acc: 0.7874 - val_loss: 0.4348 - val_acc: 0.7942\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 1355s 135ms/step - loss: 0.4212 - acc: 0.8024 - val_loss: 0.4252 - val_acc: 0.8009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb1b5a2eb0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "elegant-quantum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         10000200  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         135680    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 10,136,009\n",
      "Trainable params: 135,809\n",
      "Non-trainable params: 10,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_bd = keras.Sequential()\n",
    "simple_bd.add(embedding_layer)\n",
    "simple_bd.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "simple_bd.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "simple_bd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "billion-leadership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 2597s 259ms/step - loss: 0.5560 - acc: 0.7156 - val_loss: 0.4716 - val_acc: 0.7751\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 2225s 223ms/step - loss: 0.4475 - acc: 0.7893 - val_loss: 0.4308 - val_acc: 0.7971\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 2282s 228ms/step - loss: 0.4220 - acc: 0.8042 - val_loss: 0.4232 - val_acc: 0.8059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb0d6e1af0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_bd.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "simple_bd.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-tutorial",
   "metadata": {},
   "source": [
    "On rend trainable la couche d'embedding sur le 1er modèle pour voir si cela permet de gagner un peu d'exactitude : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].trainable = True\n",
    "# model.compile(\n",
    "#     loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    "# )\n",
    "# model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-table",
   "metadata": {},
   "source": [
    "#### Modèle basique avec embedding fasttext : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "regulation-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Done in 9.87449026107788 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "start_time = time.time()\n",
    "vectorizer = TextVectorization(max_tokens=50000, output_sequence_length=120)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples.loc[:,'cleaned_text']).batch(128)\n",
    "vectorizer.adapt(text_ds)\n",
    "print(\"--- Done in %s seconds ---\" % (time.time() - start_time))\n",
    "def _get_vocabulary():\n",
    "    keyz, valuez = vectorizer._index_lookup_layer._table_handler.data()\n",
    "    return [x.decode('latin-1') for _,x in sorted(zip(valuez, keyz))]\n",
    "vocab = _get_vocabulary()\n",
    "my_word_index = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "canadian-spare",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-3345d46196e1>:6: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  vector = np.fromstring(values, \"f\", sep=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Fasttext vectors loaded in 24.463834762573242 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ft_dict = {}\n",
    "with open('fasttext_english_twitter_100D.vec', 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    for line in f:\n",
    "        word, values = line.split(maxsplit=1)\n",
    "        vector = np.fromstring(values, \"f\", sep=\" \")\n",
    "        ft_dict[word] = vector\n",
    "print(\"---Fasttext vectors loaded in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "automotive-sister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 40476 words (9523 misses)\n",
      "--- Done in 0.06606030464172363 seconds ---\n"
     ]
    }
   ],
   "source": [
    "fasttext_matrix = create_embedding_matrix(num_tokens = len(vocab)+2,\n",
    "                                          embedding_dim = 100,\n",
    "                                          my_word_index=my_word_index,\n",
    "                                          dictionary=ft_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "split-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "fasttext_embedding_layer = Embedding(\n",
    "    len(vocab)+2,\n",
    "    100,\n",
    "    embeddings_initializer=keras.initializers.Constant(fasttext_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "residential-designer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280000, 120)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "anonymous-century",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         5000100   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 128)         117248    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 1)           129       \n",
      "=================================================================\n",
      "Total params: 5,117,477\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 5,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fasttext_single_layer = keras.Sequential()\n",
    "\n",
    "fasttext_single_layer.add(fasttext_embedding_layer)\n",
    "fasttext_single_layer.add(layers.LSTM(128, return_sequences=True))\n",
    "fasttext_single_layer.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "fasttext_single_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "actual-microwave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 1752s 175ms/step - loss: 0.6579 - acc: 0.5897 - val_loss: 0.5058 - val_acc: 0.7486\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 1780s 178ms/step - loss: 0.4893 - acc: 0.7596 - val_loss: 0.4618 - val_acc: 0.7786\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 1759s 176ms/step - loss: 0.4486 - acc: 0.7861 - val_loss: 0.4379 - val_acc: 0.7929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb053769d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_single_layer.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "fasttext_single_layer.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-supplier",
   "metadata": {},
   "source": [
    "Puis on tune la couche d'embedding : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "metropolitan-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext_single_layer.layers[0].trainable = True\n",
    "# fasttext_single_layer.compile(\n",
    "#     loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\",'AUC']\n",
    "# )\n",
    "# fasttext_single_layer.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-walter",
   "metadata": {},
   "source": [
    "on définit d'emblée notre couche d'embedding comme trainable pour les modèles suivants : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "african-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_fasttext_embedding = Embedding(\n",
    "    len(vocab)+2,\n",
    "    100,\n",
    "    embeddings_initializer=keras.initializers.Constant(fasttext_matrix),\n",
    "    trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-oakland",
   "metadata": {},
   "source": [
    "#### Essais d'une architecture combinée (LSTM + ConvNet) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "compliant-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 100)         5000100   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 120)         60120     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 120)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 112)         104384    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 112)         37744     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 112)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 108)         36396     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 108)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 109       \n",
      "=================================================================\n",
      "Total params: 5,238,853\n",
      "Trainable params: 5,238,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = trainable_fasttext_embedding(int_sequences_input)\n",
    "x = layers.Conv1D(120, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.LSTM(112, return_sequences=True)(x)\n",
    "x = layers.Conv1D(112, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(3)(x)\n",
    "x = layers.Conv1D(108, 3, activation=\"relu\",input_shape=(None,108))(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_pilot = keras.Model(int_sequences_input, preds)\n",
    "model_pilot.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "organic-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 801s 80ms/step - loss: 0.4772 - acc: 0.7665 - val_loss: 0.4041 - val_acc: 0.8163\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 777s 78ms/step - loss: 0.3953 - acc: 0.8230 - val_loss: 0.3971 - val_acc: 0.8222\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 778s 78ms/step - loss: 0.3819 - acc: 0.8313 - val_loss: 0.3975 - val_acc: 0.8243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pilot.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "history = model_pilot.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-category",
   "metadata": {},
   "source": [
    "### Essais d'architectures orientées n-grammes : \n",
    "On teste d'abord un Bi-LSTM alimenté par des couches convolutionnelles concaténées afin de mieux capturer le poids des n-grammes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "destroyed-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    5000100     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 120)    60120       embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 120)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 100, None)    0           embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 120)    0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, None, 120)    0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, None, 120)    0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 120)    0           lambda[0][0]                     \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 240)    29040       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 240)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 256)    377856      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 148)    113812      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 148)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, None, 148)    0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 144)    42768       max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 144)    0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 144)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            145         global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 5,623,841\n",
      "Trainable params: 5,623,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = trainable_fasttext_embedding(int_sequences_input)\n",
    "lamb = layers.Lambda(lambda x: K.permute_dimensions(x,(0,2,1)))(embedded_sequences)\n",
    "first = layers.Conv1D(120, 5, activation=\"relu\")(embedded_sequences)\n",
    "first = layers.MaxPooling1D(5)(first)\n",
    "second = layers.Conv1D(120, 4, activation=\"relu\")(embedded_sequences)\n",
    "second = layers.MaxPooling1D(4)(first)\n",
    "third = layers.Conv1D(120, 3, activation=\"relu\")(embedded_sequences)\n",
    "third = layers.MaxPooling1D(3)(first)\n",
    "fourth = layers.Conv1D(120, 2, activation=\"relu\")(embedded_sequences)\n",
    "fourth = layers.MaxPooling1D(2)(first)\n",
    "merged = Concatenate(axis=1)([lamb, first, second, third, fourth])\n",
    "x = layers.Dense(240, activation=\"tanh\")(merged)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "x = layers.Conv1D(148, 3, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.MaxPooling1D(3)(x)\n",
    "x = layers.Conv1D(144, 2, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "preds = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "mixed_model = keras.Model(int_sequences_input, preds)\n",
    "mixed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "interstate-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model.layers[1].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "adjustable-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 10527s 1s/step - loss: 0.4152 - acc: 0.8109 - val_loss: 0.4071 - val_acc: 0.8248\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 10659s 1s/step - loss: 0.3788 - acc: 0.8332 - val_loss: 0.4044 - val_acc: 0.8231\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 10636s 1s/step - loss: 0.3707 - acc: 0.8389 - val_loss: 0.3920 - val_acc: 0.8261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb2a8afd00>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "mixed_model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-specialist",
   "metadata": {},
   "source": [
    "#### Essai du merge de n-grammes sur 2 couches denses : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = trainable_fasttext_embedding(int_sequences_input)\n",
    "lamb = layers.Lambda(lambda x: K.permute_dimensions(x,(0,2,1)))(embedded_sequences)\n",
    "first = layers.Conv1D(120, 5, activation=\"relu\")(embedded_sequences)\n",
    "first = layers.MaxPooling1D(5)(first)\n",
    "second = layers.Conv1D(120, 4, activation=\"relu\")(embedded_sequences)\n",
    "second = layers.MaxPooling1D(4)(first)\n",
    "third = layers.Conv1D(120, 3, activation=\"relu\")(embedded_sequences)\n",
    "third = layers.MaxPooling1D(3)(first)\n",
    "fourth = layers.Conv1D(120, 2, activation=\"relu\")(embedded_sequences)\n",
    "fourth = layers.MaxPooling1D(2)(first)\n",
    "merged = Concatenate(axis=1)([lamb, first, second, third, fourth])\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(600, activation=\"sigmoid\")(merged)\n",
    "preds = layers.Dense(1,kernel_constraint=max_norm(3), activation=\"sigmoid\")(x)\n",
    "ngrams_model = keras.Model(int_sequences_input, preds)\n",
    "ngrams_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "ngrams_model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-ghana",
   "metadata": {},
   "source": [
    "### Essais d'autres architectures avancées : \n",
    "<br>On revient à notre LSTM avec 2 couches de convolution, qu'on teste avec l'embedding GloVe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "historical-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_glove_model = keras.Sequential()\n",
    "adv_glove_model.add(embedding_layer)\n",
    "adv_glove_model.add(layers.LSTM(128, return_sequences=True))\n",
    "adv_glove_model.add(layers.Conv1D(128, 5, activation=\"relu\", input_shape=(None,128)))\n",
    "adv_glove_model.add(layers.MaxPooling1D(5))\n",
    "adv_glove_model.add(layers.Conv1D(10, 2, activation=\"relu\", input_shape=(None,128)))\n",
    "adv_glove_model.add(layers.GlobalMaxPooling1D())\n",
    "adv_glove_model.add(layers.Dropout(0.3))\n",
    "adv_glove_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "adv_glove_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "compressed-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         10000200  \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, None, 128)         168448    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 10)          2570      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 10,253,277\n",
      "Trainable params: 10,253,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adv_glove_model.layers[0].trainable = True\n",
    "adv_glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "detailed-persian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 2261s 226ms/step - loss: 0.4903 - acc: 0.7652 - val_loss: 0.4325 - val_acc: 0.7984\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 2242s 224ms/step - loss: 0.4326 - acc: 0.8046 - val_loss: 0.4137 - val_acc: 0.8101\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 2244s 224ms/step - loss: 0.4136 - acc: 0.8156 - val_loss: 0.4066 - val_acc: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb2e183100>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_glove_model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-effect",
   "metadata": {},
   "source": [
    "#### Modèles avec LSTM bidirectionnel : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "suspended-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_model = keras.Sequential()\n",
    "blstm_model.add(embedding_layer)\n",
    "blstm_model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "blstm_model.add(layers.Conv1D(128, 5, activation=\"relu\", input_shape=(None,128)))\n",
    "blstm_model.add(layers.MaxPooling1D(5))\n",
    "blstm_model.add(layers.Conv1D(10, 2, activation=\"relu\", input_shape=(None,128)))\n",
    "blstm_model.add(layers.MaxPooling1D(5))\n",
    "blstm_model.add(layers.Dropout(0.1))\n",
    "blstm_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "distant-memory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         10000200  \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 256)         336896    \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, None, 128)         163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, None, 10)          2570      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, None, 1)           11        \n",
      "=================================================================\n",
      "Total params: 10,503,645\n",
      "Trainable params: 10,503,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "blstm_model.layers[0].trainable = True\n",
    "blstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "unknown-jungle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 7089s 709ms/step - loss: 0.4835 - acc: 0.7679 - val_loss: 0.4019 - val_acc: 0.8188\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 7066s 707ms/step - loss: 0.3950 - acc: 0.8235 - val_loss: 0.3905 - val_acc: 0.8246\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 6990s 699ms/step - loss: 0.3763 - acc: 0.8347 - val_loss: 0.3845 - val_acc: 0.8291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fb51e12100>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc']\n",
    ")\n",
    "blstm_model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-christianity",
   "metadata": {},
   "source": [
    "On essaie d'ajouter une couche LSTM : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "external-contribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         10000200  \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, None, 256)         336896    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, None, 128)         163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, None, 32)          8224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, None, 1)           33        \n",
      "=================================================================\n",
      "Total params: 10,903,561\n",
      "Trainable params: 903,361\n",
      "Non-trainable params: 10,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dl_blstm_model = keras.Sequential()\n",
    "dl_blstm_model.add(embedding_layer)\n",
    "dl_blstm_model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2)))\n",
    "dl_blstm_model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2)))\n",
    "dl_blstm_model.add(layers.Conv1D(128, 5, activation=\"relu\", input_shape=(None,128)))\n",
    "dl_blstm_model.add(layers.MaxPooling1D(5))\n",
    "dl_blstm_model.add(layers.Conv1D(32, 2, activation=\"relu\", input_shape=(None,128)))\n",
    "dl_blstm_model.add(layers.MaxPooling1D(3))\n",
    "dl_blstm_model.add(layers.Dropout(0.2))\n",
    "dl_blstm_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "dl_blstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "antique-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_blstm_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[['acc']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-retro",
   "metadata": {},
   "source": [
    "Pour des raisons de temps d'entraînement, la cellule ci dessous a été recopiée d'un autre notebook - il s'agit du résultat d'un entraînement du modèle ci-dessus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "desirable-macintosh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 27033s 3s/step - loss: 0.3582 - acc: 0.8491 - val_loss: 0.3881 - val_acc: 0.8294\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 27087s 3s/step - loss: 0.3462 - acc: 0.8558 - val_loss: 0.3805 - val_acc: 0.8304\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 27103s 3s/step - loss: 0.3739 - acc: 0.8587 - val_loss: 0.4111 - val_acc: 0.8304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20ee9caa100>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_blstm_model.fit(x_train, y_train, batch_size=128, epochs=3, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-eating",
   "metadata": {},
   "source": [
    "### Conclusion des essais : \n",
    "De nombreuses architectures ont été essayées, le meilleur résultat est obtenu par le réseau le plus complexe (double couche de LSTM bidirectionnel). Le temps d'entraînement est cependant élevé, le modèle assez lourd en terme de poids et ce modèle semble atteindre son best fit assez rapidement (entre 2 et 3 epochs) malgré une régularisation optimisée par de nombreux essais. <br>On choisit donc de mettre en prod un modèle plus simple et léger pour faire la mise en prod sous Azure même si les performances de ce dernier modèle pourraient justifier. \n",
    "<br>On note aussi que rendre la couche d'embedding entraînable permet d'améliorer les résultats obtenus, et on créera donc notre modèle final avec cette propriété. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-player",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
